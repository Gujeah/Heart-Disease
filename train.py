# -*- coding: utf-8 -*-
"""Mid Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oYePmPSrrivDInSQWVVXgD5uIEqX1mSv

Importing necessary labralies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve
from sklearn.tree import plot_tree
from sklearn.tree import DecisionTreeClassifier, plot_tree
import pickle



def wrangle(filepath):
  df=pd.read_csv(filepath)
  df.drop_duplicates(inplace=True)
  return df

url="https://raw.githubusercontent.com/Gujeah/Machine-learning-zoomcamp/refs/heads/main/classification%20module/heart_disease_uci.csv"

df=wrangle(url)
df.head()

"""We need to understand our datta better"""

df.shape

df.set_index('id', inplace=True)

df.head()

df.columns

"""##The column names in full names
age: Age of the patient (in years)<br>
sex: Gender of the patient (e.g., 1 = Male, 0 = Female).<br>
dataset: Source of the data<br>
cp: Chest pain type:<br>
0 = Typical Angina (chest pain related to decreased blood supply to the heart)<br>
1 = Atypical Angina (chest pain not related to heart)<br>
2 = Non-Anginal Pain (not related to the heart).<br>
3 = Asymptomatic (no chest pain)<br>
trestbps: Resting blood pressure (in mm Hg on admission to the hospital).<br>
chol: Serum cholesterol level (in mg/dL).<br>
fbs: Fasting blood sugar > 120 mg/dL (boolean):<br>
1 = True.<br>
0 = False.<br>
restecg: Resting electrocardiographic results:<br>
0 = Normal.<br>
1 = Having ST-T wave abnormality (e.g., T wave inversions and/or ST elevation or depression of > 0.05 mV).<br>
2 = Showing probable or definite left ventricular hypertrophy by Estes' criteria.<br>
thalch: Maximum heart rate achieved.<br>
exang: Exercise-induced angina (boolean):<br>
1 = Yes.<br>
0 = No.<br>
oldpeak: ST depression induced by exercise relative to rest (measured in mm).<br>
slope: The slope of the peak exercise ST segment:<br>
0 = Upsloping.<br>
1 = Flat.<br>
2 = Downsloping.<br>
ca: Number of major vessels (0–3) colored by fluoroscopy.<br>
thal: Thalassemia (blood disorder affecting hemoglobin levels):<br>
1 = Normal.<br>
2 = Fixed defect (no blood flow in some part of the heart).<br>
3 = Reversible defect (a defect in blood flow is observed, but it can be reversed).<br>
num: The target variable representing the presence of heart disease:<br>
0 = No heart disease.<br>
1–4 = Different levels of heart disease severity.<br>

As we have seen some column contents doesnt have good formating we need to change those to god format
"""

df.head(3)

categorycol= df.select_dtypes(include=["object"]).columns
for c in categorycol:
  print (df[c].value_counts())

"""As we have seen, our contents are not uniform we need to make them small letters and also remove some space"""

categorical_columns = list(df.select_dtypes(include=['object']).columns)
for c in categorical_columns:

    if pd.api.types.is_string_dtype(df[c]):
        df[c] = df[c].str.lower().str.replace(" ", "_").str.replace("-", "_")
    else:

        df[c] = df[c].astype(str).str.lower().str.replace(" ", "_").str.replace("-", "_")

"""Lets try again to check the way it is looking now"""

categorycol= df.select_dtypes(include=["object"]).columns
for c in categorycol:
  print (df[c].value_counts())

"""Lets now change the columns that have a true or false to 1 and 0"""

df.head(3)

df.fbs=(df.fbs=="true").astype(int)
df.fbs.head()

df.fbs.value_counts(normalize=True)

df.exang=(df.exang=="true").astype(int)
df.exang.head()

df.exang.value_counts(normalize=True)

df.head()





#lets check for the information of this data
df.info()

"""As we can see our data has some missing values so lets assess thoseone the percentages"""

df.isnull().sum()/len(df)*100

df.describe()

"""As we can see we have some missing number<br>
restbps, chol, thalch, exang, oldpeak has less than 10% missing numbers<br>
fbs and restecg have in between (10%-30%) missing numbers <br>
slope, ca, and thal has morethan 30% missing number

lets analyze the column with less than 10% missing values about their distribution
"""

df1=df.copy()

df1.head()

n1_variables=["chol", "thalch", "trestbps", "oldpeak"]

for n in n1_variables:
  x=df[n].values
  sns.distplot(x, color='g')
  mean=df[n].mean()
  mode=df[n].mode()
  median=df[n].median()
  plt.axvline(mean,  0, 1, color='r')
  plt.title(n)
  plt.show()





df[n1_variables].nunique()

"""Lets remove those columns with high missing values as they wont add much value<br>
Slope has 33% missing values<br>
Thai has 52.8% missing values <br>
Ca has 66.4% missing values
"""

high_missing_values= ["slope", "ca", "thal"]



df1.drop(columns=high_missing_values, inplace=True)

df1.head(4)

"""For a normal distribution lets use mean to impute missing values. for skewed lets use a median. For categorical lets try a mode. These is from those below 10%
restbps, chol, thalch, exang, oldpeak has less than 10% missing numbers
"""

n1_variables=["chol", "thalch", "trestbps", "oldpeak"]

df1['chol'].fillna(df1['chol'].median(), inplace=True)
df1['thalch'].fillna(df1['thalch'].mean(), inplace=True)
df1["trestbps"].fillna(df1["trestbps"]. median(), inplace=True)
df1['oldpeak'].fillna(df1['oldpeak'].median(), inplace=True)
df1['exang'].fillna(df1['exang'].mode()[0], inplace=True)
df1['restecg'].fillna(df1['restecg'].mode()[0], inplace=True)
df1['fbs'].fillna(df1['fbs'].mode()[0], inplace=True)

df1.isnull().sum()/len(df1)*100

df1.dtypes

df1.columns

"""Lets investigate the categorical data on how it is giving"""

categorical=[ 'sex', 'dataset', 'cp', 'fbs','restecg', 'exang']

df1[categorical].nunique()

for n in categorical:
  ax = sns.countplot(x=n, data=df1)
  for container in ax.containers:
    ax.bar_label(container)
  plt.title(n)
  plt.show()

df1.num.value_counts()

"""Converting categorical to numerical"""

df1.num.unique()

df2=df1.copy()



"""My goal is to determine about whether the person has the presence of heart disease or not , not specifically telling the person the type of that heart disease. Next colab, I will go in details on the specificity but for the sake of this colab we will do binary classification"""

df2['target_variable'] = (df2['num'] > 0).astype(int)

df2.target_variable.unique()

df2.head()

df2.drop(columns='num', inplace=True)

df2.head()

df2.restecg.value_counts()

df2.dtypes

#checking if our target variable has inbalance class
imbalance=df2['target_variable'].value_counts(normalize=True)
imbalance

"""Checking for Correlation

Lets check for the correlation of our numerical data. <br>
If we have two variables which are more correlted, we drop the one with the least predictive power
"""

df2.columns

num_variables=["age","trestbps","chol", "thalch","oldpeak"]

correlation=df2[num_variables].corr()
sns.heatmap(correlation)

"""As we can see from the heatmap above, there are no multicorrelanity so we are safe. We checked for multicorrelinearity because even for logistic regression there are also linear models

Lets not check our target variable
"""

df2["target_variable"].value_counts(normalize=True).plot(kind="bar");

df2.head(3)

df2["restecg"].value_counts(normalize=True)

dataset_pivot=pd.pivot_table(

    df2, index="dataset", values="target_variable", aggfunc=np.mean
).sort_values(by="target_variable")
dataset_pivot

majority_class_prop, minority_class_prop =df2["target_variable"].value_counts(normalize=True)
print(majority_class_prop, minority_class_prop)

dataset_pivot.plot(kind="barh", legend=None)
plt.axvline(majority_class_prop, linestyle="--", c="r", label="majority class")
plt.axvline(minority_class_prop, linestyle="--", c="g", label="minority class")
plt.legend(loc="lower right");

"""From the graph above we can literally see Switzerland and VA are way beyond the majority class unlike Hungary"""

cp_pivot=pd.pivot_table(

    df2, index="cp", values="target_variable", aggfunc=np.mean
).sort_values(by="target_variable")
cp_pivot

cp_pivot.plot(kind="barh", legend=None)
plt.axvline(majority_class_prop, linestyle="--", c="r", label="majority class")
plt.axvline(minority_class_prop, linestyle="--", c="g", label="minority class")
plt.legend(loc="lower right");

restecg_pivot=pd.pivot_table(

    df2, index="restecg", values="target_variable", aggfunc=np.mean
).sort_values(by="target_variable")
restecg_pivot

restecg_pivot.plot(kind="barh", legend=None)
plt.axvline(majority_class_prop, linestyle="--", c="r", label="majority class")
plt.axvline(minority_class_prop, linestyle="--", c="g", label="minority class")
plt.legend(loc="lower right");

"""Lets check if our data has high cardinality or not"""

df2.select_dtypes("object").nunique()

"""Conclusion: our data doesnt have a high cardinality"""



"""We should split our data to X and y for model training"""

target="target_variable"
X=df2.drop(columns=[target])
y=df2[target]


X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2, random_state=42)
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

X_train.columns

X_train.dtypes

"""Logistic Regression<br>
Building a Baseline model
"""

baseline=y_train.value_counts(normalize=True).max()
print("Baseline Accuracy:", round(baseline, 2))

"""Conclusion: Whateve the case, our model results should not be below 54%"""



from sklearn.linear_model import LogisticRegression
from category_encoders import OneHotEncoder
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline, make_pipeline
import warnings
warnings.simplefilter(action="ignore", category=FutureWarning)

"""Lets now build our Model"""

model=make_pipeline(
    OneHotEncoder(use_cat_names=True),
    LogisticRegression(max_iter=1000)
)

model.fit(X_train, y_train)

model.predict(X_train[:8])

y_pred_proba=model.predict_proba(X_train[:8])
y_pred_proba

train_accuracy =accuracy_score(y_train, model.predict(X_train))
test_accuracy = model.score(X_test, y_test)

print("Training Accuracy:", round(train_accuracy, 2))
print("Test Accuracy:", round(test_accuracy, 2))

print(classification_report(y_test, model.predict(X_test)))

ConfusionMatrixDisplay.from_estimator(model, X_test, y_test);

"""The left side is negative class while the right side is positive side

Lets now explore the Odd Ratio: Features that will be associated with the risk of heart attack
"""

features=model.named_steps["onehotencoder"].get_feature_names()
importances=model.named_steps["logisticregression"].coef_[0]

feature_importance=pd.Series(np.exp(importances), index=features).sort_values()
feature_importance.head()

odds_ratios =pd.Series(np.exp(importances), index=features).sort_values()
odds_ratios.head()

odds_ratios.tail().plot(kind="barh")
plt.xlabel("odds ratio");

"""Lets look at the least contributors"""

odds_ratios.head().plot(kind="barh")
plt.xlabel("odds ratio");



"""Using a Decision Tree Classfier

Perfoming train test split
"""

X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=42)

"""Perfoming a randomized test split"""

X_train, X_val, y_train,y_val=train_test_split(X_train,y_train, test_size=0.2, random_state=42)



"""Baseline model"""

base=y_train.value_counts(normalize=True).max()
base


model1=make_pipeline(
    # OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1),
     OneHotEncoder(use_cat_names=True),
    DecisionTreeClassifier(random_state=42)
)
model1.fit(X_train, y_train)

"""Finding the accuracy fro the training and validation data"""

training_accuracy=accuracy_score(y_train, model1.predict(X_train))
validation_accuracy=model1.score(X_val, y_val)
print(f"the accuracy of training data is {training_accuracy}")
print(f"the accuracy of validation data is {validation_accuracy}")

"""Conclusion: This model is overfitting meaning it is not generalizing well. This could be that it has taken more depth which increased the chances of overfitting. So lets explore the number of depth this model has on this data

"""

depth=model1.named_steps["decisiontreeclassifier"].get_depth()
print(f"it has a tree depth of : {depth}")

"""Lets explore hyperparameter tuning"""

depth_params=range(1,18,1)

training_acc=[]
validation_acc=[]
for d in depth_params:
  hyper_model=make_pipeline(
    # OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1),
     OneHotEncoder(use_cat_names=True),
    DecisionTreeClassifier(max_depth=d, random_state=42)
)
  hyper_model.fit(X_train, y_train)
  training_acc.append(hyper_model.score(X_train, y_train))
  validation_acc.append(hyper_model.score(X_val, y_val))
print(f"training accuracy score: {training_acc[:4]}")
print(f"validation accuracy score: {validation_acc[:4]}")

"""Lets plot to see where exactly to consider getting our depth"""

plt.plot(depth_params, training_acc, label="training_accuracy")
plt.plot(depth_params, validation_acc, label="validation_accuracy")
plt.legend();

"""So redoing our first model again using a max_depth of 2, retraining our model"""

model1=make_pipeline(
    # OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1),
     OneHotEncoder(handle_unknown="ignore",use_cat_names=True),
    DecisionTreeClassifier(max_depth=2,random_state=42)
)
model1.fit(X_train, y_train)

training_accuracy=accuracy_score(y_train, model1.predict(X_train))
validation_accuracy=model1.score(X_val, y_val)
print(f"the accuracy of training data is {training_accuracy}")
print(f"the accuracy of validation data is {validation_accuracy}")

"""Conclusion: Now it is atleast making sense. It shows that our mdel is generalizing properly

Now lets try our testing data
"""

test_acc=model1.score(X_test, y_test)
print(f"the accuracy of testing data is : {test_acc}")

"""Plotting the tree"""


fig, ax = plt.subplots(figsize=(25, 12))
feature_names = model1.named_steps["decisiontreeclassifier"].feature_names_in_

plot_tree(
    decision_tree=model1.named_steps["decisiontreeclassifier"],
    feature_names=feature_names,
    filled=True,
    rounded=True,
    proportion=True,
    max_depth=2,
    fontsize=12,
    ax=ax,
);



print(classification_report(y_test, model1.predict(X_test)))

"""[tn, fp],<br>
 [fn, tp]
"""

ConfusionMatrixDisplay.from_estimator(model1, X_test, y_test);



import sklearn
print('Area Under ROC-Curve: ', sklearn.metrics.roc_auc_score(y_test, model1.predict(X_test)))

print('Area Under ROC-Curve: ', sklearn.metrics.roc_auc_score(y_val, model1.predict(X_val)))



"""Saving the Model"""



output_file="model.pkl"
output_file

pickle.dump(model1, open(output_file, "wb"))































